SYSTEM MESSAGE:
###  Task: Generate a data science pipeline in Python 3.10 that answers a question based on a given dataset, Schema, and Data Profiling Info.
###  Input: A dataset in CSV format, a schema that describes the columns and data types of the dataset, and a data profiling info that summarizes the statistics and quality of the dataset. A question that requires data analysis or modeling to answer.
###  Output: A Python 3.10 code that performs the following steps:
# 1. **Library Importation**: Always start by importing the necessary libraries and modules required for data manipulation, analysis, and modeling. This includes libraries like `pandas`, `numpy`, `scikit-learn`, and any other specific libraries needed for the task.
# 2. **Data Loading**: Load the datasets using `pandas` CSV readers. Assign the training data to the variable `train_data=/home/ubuntu/CatDB/Experiments/data/IMDB-IJS/IMDB-IJS_train.csv` and the test data to `test_data=/home/ubuntu/CatDB/Experiments/data/IMDB-IJS/IMDB-IJS_test.csv`. Ensure that the datasets are loaded exactly as specified without any modifications or splits.
# 3. **Dataset Integrity**: Do not split the `train_data` into additional train and test sets. Use only the provided datasets for all analyses and modeling tasks.
# 4. **Schema Utilization**: The user will provide the schema of the dataset with columns named as attributes, enclosed in triple quotes, and preceded by a specific prefix. Use this schema to guide data processing.
# 5. **Feature Type Adherence**: Use the feature types specified in the schema information strictly. Avoid generalizing Python processing for extracting categorical and numerical values; adhere to the defined feature types.
# 6. **Data Preprocessing**: Analyze features based on the provided data profiling information. Implement data preprocessing tasks such as handling missing values, selecting appropriate data transformations for each column (categorical and numerical), and applying data cleaning based on the profiling information.
# 7. **Outlier Removal**: Implement outlier removal for both train and test data based on the provided data profiling information.
# 8. **Data Augmentation**: Apply data augmentation techniques based on the data profiling information. Use your knowledge to choose the best technique for the given dataset.
# 9. **Feature Engineering**: Analyze features based on the provided data profiling information and implement feature engineering tasks to enhance the dataset.
# 10. **Categorical Feature Extraction**: Do not use `select_dtypes(include=['object']).columns` for extracting categorical features. Use only the schema information to infer the categorical columns.
# 11. **Model Selection**: Select the best model based on the data profiling information. Ensure that the model choice is justified by the dataset characteristics.
# 12. **Feature and Target Selection**: Select appropriate features and target variables for the question. Consider additional columns that add new semantic information or are useful for downstream algorithms. Use appropriate scaling for columns that require transformation.
# 13. **Feature Selection**: Drop columns that may be redundant and hurt predictive performance. This helps reduce overfitting, especially in small datasets. Train the model on the dataset with the generated columns and evaluate on a holdout set.
# 14. Code formatting for all required packages and pipeline format:
# Import all required packages  
# Do not use "if __name__ == __main__:" style, use only flat mode.

# 15. Code formatting for each added column:
# (Feature name and description)
# Usefulness: (Description why this adds useful real world knowledge to classify "gender" according to dataset description and attributes.) (Some pandas code using "actor_id", "director_id", ... to add a new column for each row in df)

# 16. Code formatting for dropping columns:
# Explanation why the column XX is dropped
# df.drop(columns=['XX'], inplace=True)

# 17. 'Code formatting for training technique:
# Choose the suitable machine learning algorithm or technique (classifier).
# Explanation why the solution is selected.
trn = ... 

# 18. **Code formatting for binary classification evaluation**:
# Report evaluation based on train and test dataset
# Calculate the model accuracy, represented by a value between 0 and 1, where 0 indicates low accuracy and 1 signifies higher accuracy. Store the accuracy value in a variable labeled as "Train_Accuracy=..." and "Test_Accuracy=...".
# Calculate the model f1 score, represented by a value between 0 and 1, where 0 indicates low accuracy and 1 signifies higher accuracy. Store the f1 score value in a variable labeled as "Train_F1_score=..." and "Test_F1_score=...".
# Calculate AUC (Area Under the Curve), represented by a value between 0 and 1.
# print(f"Train_AUC:{{Train_AUC}}")
# print(f"Train_Accuracy:{{Train_Accuracy}}")   
# print(f"Train_F1_score:{{Train_F1_score}}")
# print(f"Test_AUC:{{Test_AUC}}")
# print(f"Test_Accuracy:{{Test_Accuracy}}")   
# print(f"Test_F1_score:{{Test_F1_score}}")

# 19. **Preprocessing for Unseen Values**: To avoid runtime errors for unseen values in the target feature, perform preprocessing based on the union of train and test datasets.
# 20. **Relevance Check**: If the question is not relevant to the dataset or the task, output "Insufficient information".
# 21. Each codeblock ends with "```end" and starts with "```python".
---------------------------------------
PROMPT TEXT:
Schema, and Data Profiling Info:
"""
# "actor_id" (int), distinct-count [658437], min-value [3], max-value [845463], median-value [366323], mean-value [384048.484]
# "director_id" (int), distinct-count [53069], min-value [2], max-value [88799], median-value [43556], mean-value [43755.073]
# "movie_id" (int), distinct-count [173394], min-value [2], max-value [378614], median-value [189233], mean-value [188785.839]
# "year" (int), categorical-values [2000,2001,1998,and 113 more]
# "last_name_x" (str), distinct-count [221976]
# "genre_y" (str), categorical-values [Comedy,Action,Crime,and 17 more]
# "last_name_y" (str), distinct-count [33091]
# "first_name_x" (str), distinct-count [94490]
# "name" (str), distinct-count [160625]
# "first_name_y" (str), distinct-count [16460]
# "genre_x" (str), categorical-values [Comedy,Action,Thriller,and 17 more]
# "role" (str), distinct-count [870048]
# "prob" (float), distinct-count [1796], min-value [0.002], max-value [1], median-value [0.133], mean-value [0.248]
# "rank" (float), distinct-count [90], min-value [1], max-value [9.9], median-value [6.2], mean-value [6.043]
# "gender" (str, **This is a target column**), categorical-values [M,F]
"""

### Do missing values imputation semantically for the following numerical columns:
	Columns: "rank"


### Predict the missing values semantically for the following string/object columns:
	Columns: "last_name_x","name","role"

### Transformer the following columns by Adaptive Binning or Scaler method (do it base on the min-max, mean, and median values are in the "Schema, and Data Profiling Info"):
 	# Columns: "actor_id","director_id","movie_id","prob","rank"

### Transformer the categorical data for the following (e.g., One-Hot Encoding, Ordinal Encoder, Polynomial Encoder, Count Encoder, ... ) columns:
	# Columns: "year","genre_y","genre_x"

### Dataset Attribute:
# Number of samples (rows) in training dataset: 30530313

### Dataset is a structured/tabular data, select a high performance ML model. For example, Gradient Boosting Machines (e.g., XGBoost, LightGBM, ...), RandomForest, ...

### Question: Provide a complete pipeline code that can be executed in a multi-threaded environment.