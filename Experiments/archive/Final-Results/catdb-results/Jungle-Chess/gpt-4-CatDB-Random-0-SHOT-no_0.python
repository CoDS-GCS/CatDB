# ```python
# Import all required packages
import pandas as pd
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, log_loss
from sklearn.pipeline import Pipeline
# ```end

# ```python
# Load the training and test datasets
train_data = pd.read_csv("../../../data/Jungle-Chess/Jungle-Chess_train.csv")
test_data = pd.read_csv("../../../data/Jungle-Chess/Jungle-Chess_test.csv")
# ```end

# ```python
# Perform data cleaning and preprocessing
# As per the given schema, there are no missing values and all the columns are of integer type. So, no data cleaning is required.
# ```end

# ```python
# Perform feature processing
# As per the given schema, all the columns are of integer type. So, we will use OneHotEncoder for all the columns except the target column 'class'.

# Define preprocessor
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), ['black_piece0_file', 'white_piece0_rank', 'white_piece0_strength', 'black_piece0_rank', 'black_piece0_strength', 'white_piece0_file']),
        ('cat', OneHotEncoder(), ['black_piece0_file', 'white_piece0_rank', 'white_piece0_strength', 'black_piece0_rank', 'black_piece0_strength', 'white_piece0_file'])
    ])
# ```end

# ```python
# Select the appropriate features and target variables for the question
X_train = train_data.drop('class', axis=1)
y_train = train_data['class']
X_test = test_data.drop('class', axis=1)
y_test = test_data['class']
# ```end

# ```python
# Perform drops columns, if these may be redundant and hurt the predictive performance of the downstream classifier (Feature selection)
# As per the given schema, all the columns seem to be important for the prediction of 'class'. So, no columns are dropped.
# ```end

# ```python
# Choose the suitable machine learning algorithm or technique (classifier)
# RandomForestClassifier is chosen because it is a versatile and widely used algorithm that can handle both categorical and numerical features. It also has the feature of handling overfitting.

# Define classifier
classifier = RandomForestClassifier(max_leaf_nodes=500)

# Combine preprocessor and classifier into a pipeline
model = Pipeline(steps=[('preprocessor', preprocessor),
                        ('classifier', classifier)])
# ```end

# ```python
# Train the model
model.fit(X_train, y_train)
# ```end

# ```python
# Report evaluation based on train and test dataset
y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)

Train_Accuracy = accuracy_score(y_train, y_train_pred)
Test_Accuracy = accuracy_score(y_test, y_test_pred)

Train_Log_loss = log_loss(y_train, y_train_pred)
Test_Log_loss = log_loss(y_test, y_test_pred)

print(f"Train_Accuracy:{Train_Accuracy}")   
print(f"Train_Log_loss:{Train_Log_loss}") 
print(f"Test_Accuracy:{Test_Accuracy}")   
print(f"Test_Log_loss:{Test_Log_loss}")
# ```end