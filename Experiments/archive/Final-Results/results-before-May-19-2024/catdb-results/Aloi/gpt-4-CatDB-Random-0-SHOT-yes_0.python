# ```python
# Import all required packages
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import r2_score, mean_squared_error
# ```end

# ```python
# Load the training and test datasets
train_data = pd.read_csv('../../../data/Aloi/Aloi_train.csv')
test_data = pd.read_csv('../../../data/Aloi/Aloi_test.csv')
# ```end

# ```python
# Perform data cleaning and preprocessing
# Here we assume that the data is clean and does not contain any missing or erroneous values.
# If this is not the case, appropriate data cleaning steps should be added here.
# ```end

# ```python
# Perform feature processing
# Select the columns that need to be scaled
scale_columns = ['76','119','114','91','113','15','21','67','38','80','66','31','110','120','34','58','109','55','29','62','89','111','84','6','35','16','124','96','104','123','43','92','75','5','68','73','103','17','57','14','93','98','70','27','30','47','54','116','118','81','41','64','8','3','61','50','44','79','97','51','126','56','60','125','88','53','69','100','63','26','18','78','1','87','25','105','4','86','99','46','13','42','106','90','40','115','7','127','83','39','94','101','9','112','22','2','71','117','33','107','10','20','48','108','11','36','49','28','52','19','23','74','77','32','37','102','122','12','95','24','45','65','72','59','85','0','121','82']
# Apply MinMaxScaler
scaler = MinMaxScaler()
train_data[scale_columns] = scaler.fit_transform(train_data[scale_columns])
test_data[scale_columns] = scaler.transform(test_data[scale_columns])

# Select the columns that need to be one-hot encoded
encode_columns = ['76','119','114','91','113','15','21','67','38','80','66','31','110','120','34','58','109','55','29','62','89','111','84','6','35','16','124','96','104','123','43','92','75','5','68','73','103','17','57','14','93','98','70','27','30','47','54','116','118','81','41','64','8','3','61','50','44','79','97','51','126','56','60','125','88','53','69','100','63','26','18','78','1','87','25','105','4','86','99','46','13','42','106','90','40','115','7','127','83','39','94','101','9','112','22','2','71','117','33','107','10','20','48','108','11','36','49','28','52','19','23','74','77','32','37','102','122','12','95','24','45','65','72','59','85','0','121','82']
# Apply OneHotEncoder
encoder = OneHotEncoder(drop='first', sparse=False)
train_encoded = pd.DataFrame(encoder.fit_transform(train_data[encode_columns]))
test_encoded = pd.DataFrame(encoder.transform(test_data[encode_columns]))

# Merge the non-encoded and encoded datasets
train_data = pd.concat([train_data.drop(encode_columns, axis=1), train_encoded], axis=1)
test_data = pd.concat([test_data.drop(encode_columns, axis=1), test_encoded], axis=1)
# ```end

# ```python
# Select the appropriate features and target variables for the question
# Here we assume that the target variable is 'target' and all other columns are features.
X_train = train_data.drop('target', axis=1)
y_train = train_data['target']
X_test = test_data.drop('target', axis=1)
y_test = test_data['target']
# ```end

# ```python
# Choose the suitable machine learning algorithm or technique (regressor)
# We choose RandomForestClassifier because it is a versatile and widely used algorithm that can handle both categorical and numerical features.
# It also has features to mitigate overfitting.
clf = RandomForestClassifier(max_leaf_nodes=500)
clf.fit(X_train, y_train)
# ```end

# ```python
# Report evaluation based on train and test dataset
y_train_pred = clf.predict(X_train)
y_test_pred = clf.predict(X_test)

Train_R_Squared = r2_score(y_train, y_train_pred)
Train_RMSE = np.sqrt(mean_squared_error(y_train, y_train_pred))
Test_R_Squared = r2_score(y_test, y_test_pred)
Test_RMSE = np.sqrt(mean_squared_error(y_test, y_test_pred))

print(f"Train_R_Squared:{Train_R_Squared}")   
print(f"Train_RMSE:{Train_RMSE}") 
print(f"Test_R_Squared:{Test_R_Squared}")   
print(f"Test_RMSE:{Test_RMSE}") 
# ```end