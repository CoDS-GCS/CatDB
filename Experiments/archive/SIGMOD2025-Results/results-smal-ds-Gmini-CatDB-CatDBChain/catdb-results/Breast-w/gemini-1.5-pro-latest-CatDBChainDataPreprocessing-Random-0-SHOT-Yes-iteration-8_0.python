# ```python
import pandas as pd
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.multiprocessing import Pool

categorical_cols = ['Normal_Nucleoli', 'Bland_Chromatin', 'Clump_Thickness', 'Cell_Shape_Uniformity',
                   'Bare_Nuclei', 'Cell_Size_Uniformity', 'Marginal_Adhesion', 'Mitoses', 'Single_Epi_Cell_Size',
                   'Class']

train_data = pd.read_csv('../../../data/Breast-w/Breast-w_train.csv')
test_data = pd.read_csv('../../../data/Breast-w/Breast-w_test.csv')

encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)

preprocessor = ColumnTransformer(
    transformers=[
        ('cat', encoder, categorical_cols)
    ],
    remainder='passthrough'
)

pipeline = Pipeline([
    ('preprocessor', preprocessor)
])

def process_data(data):
    return pipeline.fit_transform(data)

with Pool() as pool:
    # Process the training and test data in parallel
    train_data_processed, test_data_processed = pool.map(process_data, [train_data, test_data])
# ```end