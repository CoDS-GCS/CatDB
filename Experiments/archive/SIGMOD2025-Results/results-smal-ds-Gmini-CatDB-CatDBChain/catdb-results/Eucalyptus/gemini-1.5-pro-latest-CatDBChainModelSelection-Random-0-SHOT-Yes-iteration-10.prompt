SYSTEM MESSAGE:
###  Task: Select an appropriate classifier Machine Learning model for the question.
###  Input: first draft version of pipline with a Data Preprocessing and Feature Engineering task enclosed in "<CODE> pipline code will be here. </CODE>", and a schema that describes the columns and data types of the dataset, and a data profiling info that summarizes the statistics and quality of the dataset.
###  Output: A modified Python 3.10 code with a Machine Learning algorithm task that performs the following steps:
#1 : Select classifier algorithm (such as RandomForestClassifier/XGBoost and so on) predicting "Utility".
#2 : Select a suitable hyperparameters for the selected algorithm. If the algorithm is RandomForestClassifier then pass max_leaf_nodes=500 as parameter.
#3 : Code formatting for multiclass classification evaluation:
```python
# Report evaluation based on train and test dataset
# Calculate the model accuracy, represented by a value between 0 and 1, where 0 indicates low accuracy and 1 signifies higher accuracy. Store the accuracy value in a variable labeled as "Train_Accuracy=..." and "Test_Accuracy=...".
# Calculate the model log loss, a lower log-loss value means better predictions. Store the  log loss value in a variable labeled as "Train_Log_loss=..." and "Test_Log_loss=...".
# Calculate AUC_OVO (Area Under the Curve One-vs-One), represented by a value between 0 and 1.
# Calculate AUC_OVR (Area Under the Curve One-vs-Rest), represented by a value between 0 and 1.
# Print the train AUC One-vs-One result: print(f"Train_AUC_OVO:{Train_AUC_OVO}")
# Print the train AUC One-vs-Rest result: print(f"Train_AUC_OVR:{Train_AUC_OVR}")
# Print the train accuracy result: print(f"Train_Accuracy:{Train_Accuracy}")   
# Print the train log loss result: print(f"Train_Log_loss:{Train_Log_loss}") 
# Print the test AUC One-vs-One result: print(f"Test_AUC_OVO:{Test_AUC_OVO}")
# Print the test AUC One-vs-Rest result: print(f"Test_AUC_OVR:{Test_AUC_OVR}")
# Print the test accuracy result: print(f"Test_Accuracy:{Test_Accuracy}")   
# Print the test log loss result: print(f"Test_Log_loss:{Test_Log_loss}")
```end

#4 : Don't report validation evaluation. We don't need it.
#5 : Each codeblock ends with "```end" and starts with "```python".
#6 : Don't use "if __name__ == '__main__':" style, use only flat mode.
---------------------------------------
PROMPT TEXT:
###  Description of the dataset:

**Eucalyptus Soil Conservation**  
The objective was to determine which seedlots in a species are best for soil conservation in seasonally dry hill country. Determination is found by measurement of height, diameter by height, survival, and other contributing factors. 
 
It is important to note that eucalypt trial methods changed over time; earlier trials included mostly 15 - 30cm tall seedling grown in peat plots and the later trials have included mostly three replications of eight trees grown. This change may contribute to less significant results.

Experimental data recording procedures which require noting include:
 - instances with no data recorded due to experimental recording procedures
   require that the absence of a species from one replicate at a site was
   treated as a missing value, but if absent from two or more replicates at a
   site the species was excluded from the site's analyses.
 - missing data for survival, vigour, insect resistance, stem form, crown form
   and utility especially for the data recorded at the Morea Station; this 
   could indicate the death of species in these areas or a lack in collection
   of data.  



 Attribute Information  
 
  1.  Abbrev - site abbreviation - enumerated
  2.  Rep - site rep - integer
  3.  Locality - site locality in the North Island - enumerated
  4.  Map_Ref - map location in the North Island - enumerated
  5.  Latitude - latitude approximation - enumerated
  6.  Altitude - altitude approximation - integer
  7.  Rainfall - rainfall (mm pa) - integer
  8.  Frosts - frosts (deg. c) - integer
  9.  Year - year of planting - integer
  10. Sp - species code - enumerated
  11. PMCno - seedlot number - integer
  12. DBH - best diameter base height (cm) - real
  13. Ht - height (m) - real
  14. Surv - survival - integer
  15. Vig - vigour - real
  16. Ins_res - insect resistance - real
  17. Stem_Fm - stem form - real
  18. Crown_Fm - crown form - real
  19. Brnch_Fm - branch form - real
  Class:
  20. Utility - utility rating - enumerated



 Relevant papers

Bulluch B. T., (1992) Eucalyptus Species Selection for Soil Conservation in Seasonally Dry Hill Country - Twelfth Year Assessment  New Zealand Journal of Forestry Science 21(1): 10 - 31 (1991)  

Kirsten Thomson and Robert J. McQueen (1996) Machine Learning Applied to Fourteen Agricultural Datasets. University of Waikato Research Report  
https://www.cs.waikato.ac.nz/ml/publications/1996/Thomson-McQueen-96.pdf + the original publication:


### <CODE>
# ```python
import pandas as pd
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.compose import TransformedTargetRegressor
from sklearn.linear_model import LinearRegression

categorical_features = ['Stem_Fm', 'Vig', 'Brnch_Fm', 'Ins_res', 'Crown_Fm', 'Altitude', 'Rep', 'Rainfall', 'Map_Ref', 
                       'Locality', 'Frosts', 'Sp', 'Latitude', 'Year', 'Abbrev']
numerical_features = ['DBH', 'Ht', 'Surv']

numerical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features)
    ])

pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', TransformedTargetRegressor(regressor=LinearRegression(), transformer=StandardScaler()))
])
# ```end
</CODE>

### Schema, and Data Profiling Info:
"""
# DBH (float), distinct-count [603]
# Stem_Fm (float), categorical-values [2.5999999046325684,5.0,3.0,3.5,3.200000047683716,3.299999952316284,2.5,2.799999952316284,4.0,1.0,2.0,3.400000095367432,2.299999952316284,0.0,4.199999809265137,2.700000047683716,3.700000047683716,4.300000190734863,4.800000190734863,3.799999952316284,and 6 more]
# Surv (float), distinct-count [47]
# Vig (float), categorical-values [3.5999999046325684,5.0,2.5,3.5,2.700000047683716,4.0,3.0,3.299999952316284,1.899999976158142,1.0,1.5,2.0,4.5,4.599999904632568,2.799999952316284,3.700000047683716,2.299999952316284,1.7999999523162842,4.099999904632568,2.900000095367432,and 13 more]
# Ht (float), distinct-count [531]
# Brnch_Fm (float), categorical-values [2.5,4.5,3.5,3.0,4.0,2.799999952316284,2.0,1.0,0.5,4.300000190734863,1.2000000476837158,1.5,3.200000047683716,3.299999952316284,0.0,3.700000047683716,3.0999999046325684,3.400000095367432,2.900000095367432,1.7000000476837158,and 8 more]
# Ins_res (float), categorical-values [3.799999952316284,4.0,2.0,3.0,3.5,2.0999999046325684,1.0,1.5,2.5,2.299999952316284,1.2000000476837158,3.200000047683716,2.700000047683716,1.7999999523162842,2.799999952316284,4.5,3.299999952316284,1.7000000476837158,3.700000047683716,2.5999999046325684,and 8 more]
# Crown_Fm (float), categorical-values [2.5999999046325684,4.0,3.299999952316284,3.5,3.0,2.0,1.0,4.5,2.5,1.7999999523162842,3.200000047683716,4.199999809265137,1.5,3.700000047683716,3.900000095367432,3.400000095367432,3.799999952316284,4.099999904632568,4.300000190734863,2.900000095367432,and 9 more]
# Altitude (int), categorical-values [150,100,180,220,130,160,200,300,70]
# Rep (int), categorical-values [1,3,2,22]
# PMCno (int), distinct-count [85]
# Rainfall (int), categorical-values [1300,850,1080,1050,1000,1400,1200,1750,900,1250]
# Map_Ref (int), categorical-values [10,0,4,8,2,5,6,1,9,12,3,11,13,7]
# Locality (int), categorical-values [6,0,4,5,2,1,7,3]
# Frosts (int), categorical-values [-2,-3]
# Sp (int), categorical-values [23,5,6,14,8,22,18,12,24,3,16,13,4,19,1,7,15,11,2,26,and 7 more]
# Latitude (int), categorical-values [9,0,4,7,2,5,1,8,11,3,10,6]
# Year (int), categorical-values [1983,1980,1981,1982,1986]
# Abbrev (int), categorical-values [11,0,12,9,2,5,6,1,8,10,14,4,3,13,15,7]
# Utility (int, **This is a target column**), categorical-values [3,4,2,1,0]
"""

### Encode categorical values by "on-hot-encoder" for the following columns:
	# Columns: Stem_Fm,Vig,Brnch_Fm,Ins_res,Crown_Fm,Altitude,Rep,Rainfall,Map_Ref,Locality,Frosts,Utility,Sp,Latitude,Year,Abbrev

### Dataset Attribute:
# Number of samples (rows) in training dataset: 736

### Question: Provide a complete pipeline code that can be executed in a multi-threaded environment.