SYSTEM MESSAGE:
###  Task: Generate a data science pipeline in Python 3.10 that answers a question based on a given dataset, Schema, and Data Profiling Info.
###  Input: A dataset in CSV format, a schema that describes the columns and data types of the dataset, and a data profiling info that summarizes the statistics and quality of the dataset. A question that requires data analysis or modeling to answer.
###  Output: A Python 3.10 code that performs the following steps:
# 1: Import the necessary libraries and modules.
# 2: Load the training and test datasets. For the training data, utilize the variable """train_data=/home/ubuntu/CatDB/Experiments/data/Midwest-Survey/Midwest-Survey_train_clean.csv""", and for the test data, employ the variable """test_data=/home/ubuntu/CatDB/Experiments/data/Midwest-Survey/Midwest-Survey_test_clean.csv""". Utilize pandas CSV readers to load the datasets.
# 3: Do not split the train_data into train and test sets. Use only the given datasets.
# 4: The user will provide the Schema, and Data Profiling Info of the dataset with columns appropriately named as attributes, enclosed in triple quotes, and preceded by the prefix "Schema, and Data Profiling Info:".
# 5: Explicitly analyze feature based on the provided data profiling information and implement data preprocessing tasks: i) handel missing values, ii) select appropriate data transfer for each column specifically (categorical values and numerical values based on min, max, mean, median values), iii) apply data cleaning based on data profiling information.
# 6: Explicitly implement Outlier removal for Train and Test data based on the provided data profiling information.
# 7: Explicitly do data augmentation techniques based on the data profiling information (use your knowledge to chose best technique).
# 8: Explicitly analyze feature based on the provided data profiling information and implement feature engineering task.
# 9: Explicitly select a best model based on the data profiling information.
# 10: Select the appropriate features and target variables for the question. Additional columns add new semantic information, additional columns that are useful for a downstream algorithm predicting "Location (Census Region)". They can e.g. be feature combinations, transformations, aggregations where the new column is a function of the existing columns. Use appropriate scale factor for columns are need to transfer.
# 11: Perform drops columns, if these may be redundant and hurt the predictive performance of the downstream classifier (Feature selection). Dropping columns may help as the chance of overfitting is lower, especially if the dataset is small. The Location (Census Region) will be trained on the dataset with the generated columns and evaluated on a holdout set.
# 12: In order to avoid runtime error for unseen value on the target feature, do preprocessing based on union of train and test dataset.
# 13: Code formatting for all required packages and pipeline format:
# Import all required packages  
# Do not use "if __name__ == __main__:" style, use only flat mode.

# 14: Code formatting for each added column:
# (Feature name and description)
# Usefulness: (Description why this adds useful real world knowledge to classify "Location (Census Region)" according to dataset description and attributes.) (Some pandas code using "Gender", "North Dakota in MW?", ... to add a new column for each row in df)

# 15: Code formatting for dropping columns:
# Explanation why the column XX is dropped
# df.drop(columns=['XX'], inplace=True)

# 16: 'Code formatting for training technique:
# Choose the suitable machine learning algorithm or technique (classifier).
# Explanation why the solution is selected.
trn = ... 

# 17: Code formatting for multiclass classification evaluation:
# Report evaluation based on train and test dataset
# Calculate the model accuracy, represented by a value between 0 and 1, where 0 indicates low accuracy and 1 signifies higher accuracy. Store the accuracy value in a variable labeled as "Train_Accuracy=..." and "Test_Accuracy=...".
# Calculate the model log loss, a lower log-loss value means better predictions. Store the  log loss value in a variable labeled as "Train_Log_loss=..." and "Test_Log_loss=...".
# Calculate AUC_OVO (Area Under the Curve One-vs-One), represented by a value between 0 and 1.
# Calculate AUC_OVR (Area Under the Curve One-vs-Rest), represented by a value between 0 and 1.
# print(f"Train_AUC_OVO:{{Train_AUC_OVO}}")
# print(f"Train_AUC_OVR:{{Train_AUC_OVR}}")
# print(f"Train_Accuracy:{{Train_Accuracy}}")   
# print(f"Train_Log_loss:{{Train_Log_loss}}") 
# print(f"Test_AUC_OVO:{{Test_AUC_OVO}}")
# print(f"Test_AUC_OVR:{{Test_AUC_OVR}}")
# print(f"Test_Accuracy:{{Test_Accuracy}}")   
# print(f"Test_Log_loss:{{Test_Log_loss}}")

# 18: If the question is not relevant to the dataset or the task, the output should be: "Insufficient information."
# 19: Do not report validation evaluation. We do not need it.
# 20: Each codeblock ends with "```end" and starts with "```python".
---------------------------------------
PROMPT TEXT:
Schema, and Data Profiling Info:
"""
# "Gender" (str), categorical-values [Female,Male]
# "North Dakota in MW?" (str), categorical-values [No,Yes]
# "Wisconsin in MW?" (str), categorical-values [Yes,No]
# "Household Income" (str), categorical-values [$150,000+,$100,000 - $149,999,$50,000 - $99,999,and 2 more]
# "West Virginia in MW?" (str), categorical-values [No,Yes]
# "In your own words, what would you call the part of the country you live in now?" (str), categorical-values [California,West coast,northwest,and 1005 more]
# "Kansas in MW?" (str), categorical-values [Yes,No]
# "Missouri in MW?" (str), categorical-values [Yes,No]
# "Colorado in MW?" (str), categorical-values [No,Yes]
# "Indiana in MW?" (str), categorical-values [Yes,No]
# "Oklahoma in MW?" (str), categorical-values [No,Yes]
# "Pennsylvania in MW?" (str), categorical-values [No,Yes]
# "Age" (str), categorical-values [> 60,45-60,30-44,and 1 more]
# "Illinois in MW?" (str), categorical-values [Yes,No]
# "Iowa in MW?" (str), categorical-values [Yes,No]
# "Education" (str), categorical-values [Graduate degree,Associate or bachelor degree,Some college,and 2 more]
# "Ohio in MW?" (str), categorical-values [Yes,No]
# "South Dakota in MW?" (str), categorical-values [Yes,No]
# "Minnesota in MW?" (str), categorical-values [Yes,No]
# "Arkansas in MW?" (str), categorical-values [No,Yes]
# "Montana in MW?" (str), categorical-values [No,Yes]
# "Wyoming in MW?" (str), categorical-values [No,Yes]
# "Kentucky in MW?" (str), categorical-values [No,Yes]
# "Personally identification as a Midwesterner?" (str), categorical-values [Not at all,Not much,Some,and 1 more]
# "Michigan in MW?" (str), categorical-values [Yes,No]
# "Nebraska in MW?" (str), categorical-values [Yes,No]
# "RespondentID" (int), distinct-count [2778], min-value [3120591286], max-value [3126807211], median-value [3122786517.5], mean-value [3122898697.202]
# "ZIP Code" (int), distinct-count [2089], min-value [170], max-value [773454], median-value [52404], mean-value [51698.395]
# "Location (Census Region)" (str, **This is a target column**), categorical-values [Pacific,Mountain,West South Central,and 6 more]
"""

### Do missing values imputation semantically for the following numerical columns:
	Columns: "ZIP Code"


### Predict the missing values semantically for the following categorical columns:
	Columns: "Gender","Household Income","In your own words, what would you call the part of the country you live in now?","Age","Education","Location (Census Region)"


### Transformer the following columns by Adaptive Binning or Scaler method (do it base on the min-max, mean, and median values are in the "Schema, and Data Profiling Info"):
 	# Columns: "RespondentID","ZIP Code"

### Transformer the categorical data for the following (e.g., One-Hot Encoding, Ordinal Encoder, Polynomial Encoder, Count Encoder, ... ) columns:
	# Columns: "Gender","North Dakota in MW?","Wisconsin in MW?","Household Income","West Virginia in MW?","In your own words, what would you call the part of the country you live in now?","Kansas in MW?","Missouri in MW?","Colorado in MW?","Indiana in MW?","Oklahoma in MW?","Pennsylvania in MW?","Age","Illinois in MW?","Iowa in MW?","Education","Ohio in MW?","South Dakota in MW?","Minnesota in MW?","Arkansas in MW?","Montana in MW?","Wyoming in MW?","Kentucky in MW?","Personally identification as a Midwesterner?","Michigan in MW?","Nebraska in MW?"

### Dataset Attribute:
# Number of samples (rows) in training dataset: 2778

### Dataset is a structured/tabular data, select a high performance ML model. For example, Gradient Boosting Machines (e.g., XGBoost, LightGBM, ...), RandomForest, ...

### Question: Provide a complete pipeline code that can be executed in a multi-threaded environment.