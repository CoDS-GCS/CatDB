# Import all required packages
import pandas as pd
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score
from sklearn.preprocessing import LabelEncoder

# Load the training and test datasets
train_data = pd.read_csv("data/dataset_2_rnc/dataset_2_rnc_train.csv")
test_data = pd.read_csv("data/dataset_2_rnc/dataset_2_rnc_test.csv")

# Remove low ration, static, and unique columns by getting statistic values
train_data = train_data.loc[:, train_data.apply(pd.Series.nunique) != 1]
for col in train_data.columns:
    if len(train_data[col].unique()) == len(train_data):
        train_data.drop(col,inplace=True,axis=1)

# Feature name and description: c_8_c_11_ratio
# Usefulness: This feature might capture some interaction between 'c_8' and 'c_11' that could be useful for predicting 'c_21'.
train_data['c_8_c_11_ratio'] = train_data['c_8'] / train_data['c_11']
test_data['c_8_c_11_ratio'] = test_data['c_8'] / test_data['c_11']

# Explanation why the column c_14 is dropped
# c_14 has a high percentage of missing values (0.82%) and hence, it might not be very useful for our model.
train_data.drop(columns=['c_14'], inplace=True)
test_data.drop(columns=['c_14'], inplace=True)

# Use a LabelEncoder technique
# Explanation why the solution is selected: LabelEncoder is used to transform non-numerical labels to numerical labels. 
# Numerical labels are always between 0 and n_classes-1. 
# We need to convert our target column 'c_21' to numerical labels for our binary classification model.
le = LabelEncoder()
train_data['c_21'] = le.fit_transform(train_data['c_21'])
test_data['c_21'] = le.transform(test_data['c_21'])

# Convert all categorical variables into numerical ones
for col in train_data.columns:
    if train_data[col].dtype == 'object':
        train_data[col] = le.fit_transform(train_data[col])
for col in test_data.columns:
    if test_data[col].dtype == 'object':
        test_data[col] = le.transform(test_data[col])

# Use a RandomForestClassifier technique
# Explanation why the solution is selected: RandomForestClassifier is a meta estimator that fits a number of decision tree classifiers 
# on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. 
# It can handle a large number of features, and it's helpful for estimating which of your variables are important in the underlying data being modeled.
X_train = train_data.drop('c_21', axis=1)
y_train = train_data['c_21']
X_test = test_data.drop('c_21', axis=1)
y_test = test_data['c_21']

clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train, y_train)

# Report evaluation based on only test dataset
y_pred = clf.predict(X_test)
Accuracy = accuracy_score(y_test, y_pred)
F1_score = f1_score(y_test, y_pred)

print(f"Accuracy:{Accuracy}")   
print(f"F1_score:{F1_score}")