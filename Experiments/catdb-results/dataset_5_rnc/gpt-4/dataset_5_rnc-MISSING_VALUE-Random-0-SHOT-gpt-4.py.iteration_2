# Import all required packages
import pandas as pd
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score

# Load the training and test datasets
train_data = pd.read_csv("data/dataset_5_rnc/dataset_5_rnc_train.csv")
test_data = pd.read_csv("data/dataset_5_rnc/dataset_5_rnc_test.csv")

# Feature name and description: c_1_c_4_ratio
# Usefulness: This feature represents the ratio of c_1 to c_4. It might add useful real world knowledge to classify 'c_9' as it combines information from two columns.
train_data['c_1_c_4_ratio'] = train_data['c_1'] / train_data['c_4']
test_data['c_1_c_4_ratio'] = test_data['c_1'] / test_data['c_4']

# Replace infinities with NaN
train_data.replace([np.inf, -np.inf], np.nan, inplace=True)
test_data.replace([np.inf, -np.inf], np.nan, inplace=True)

# Fill NaN values with 0
train_data.fillna(0, inplace=True)
test_data.fillna(0, inplace=True)

# c_2 is dropped because it has a high correlation with c_1 and c_4, which might lead to multicollinearity in the model.
train_data.drop(columns=['c_2'], inplace=True)
test_data.drop(columns=['c_2'], inplace=True)

# Use a RandomForestClassifier technique
# RandomForestClassifier is selected because it is a robust and versatile classifier that can handle both numerical and categorical data. It also has features to handle overfitting.
clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
X_train = train_data.drop(columns=['c_9'])
y_train = train_data['c_9']
clf.fit(X_train, y_train)

# Report evaluation based on only test dataset
X_test = test_data.drop(columns=['c_9'])
y_test = test_data['c_9']
y_pred = clf.predict(X_test)

# Calculate the model accuracy
Accuracy = accuracy_score(y_test, y_pred)

# Calculate the model f1 score
F1_score = f1_score(y_test, y_pred)

# Print the accuracy result
print(f"Accuracy:{Accuracy}")

# Print the f1 score result
print(f"F1_score:{F1_score}")