
The dataframe `df` is loaded and in memory. Columns are also named attributes.
Description of the dataset in `df` (column dtypes might be inaccurate):
"
**Breast Cancer Wisconsin (Original) Data Set.** Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. The target feature records the prognosis (malignant or benign). [Original data available here](ftp://ftp.cs.wisc.edu/math-prog/cpo-dataset/machine-learn/cancer/)  

Current dataset was adapted to ARFF format from the UCI version. Sample code ID's were removed.  

! Note that there is also a related Breast Cancer Wisconsin (Diagnosis) Data Set with a different set of features, better known as [wdbc](https://www.openml.org/d/1510).



 Citation request  

This breast cancer database was obtained from the University of Wisconsin Hospitals, Madison from Dr. William H. Wolberg.  If you publish results when using this database, then please include this information in your acknowledgments.  Also, please cite one or more of:

   1. O. L. Mangasarian and W. H. Wolberg: "Cancer diagnosis via linear 
      programming", SIAM News, Volume 23, Number 5, September 1990, pp 1 & 18.

   2. William H. Wolberg and O.L. Mangasarian: "Multisurface method of 
      pattern separation for medical diagnosis applied to breast cytology", 
      Proceedings of the National Academy of Sciences, U.S.A., Volume 87, 
      December 1990, pp 9193-9196.

   3. O. L. Mangasarian, R. Setiono, and W.H. Wolberg: "Pattern recognition 
      via linear programming: Theory and application to medical diagnosis", 
      in: "Large-scale numerical optimization", Thomas F. Coleman and Yuying
      Li, editors, SIAM Publications, Philadelphia 1990, pp 22-30.

   4. K. P. Bennett & O. L. Mangasarian: "Robust linear programming 
      discrimination of two linearly inseparable sets", Optimization Methods
      and Software 1, 1992, 23-34 (Gordon & Breach Science Publishers)."

Columns in `df` (true feature dtypes listed here, categoricals encoded as int):
Clump_Thickness (float64): NaN-freq [0.0%], Samples [5.0, 10.0, 5.0, 2.0, 4.0, 5.0, 1.0, 10.0, 1.0, 8.0]
Cell_Size_Uniformity (float64): NaN-freq [0.0%], Samples [1.0, 2.0, 6.0, 1.0, 1.0, 3.0, 1.0, 10.0, 1.0, 10.0]
Cell_Shape_Uniformity (float64): NaN-freq [0.0%], Samples [1.0, 2.0, 5.0, 1.0, 1.0, 5.0, 1.0, 10.0, 1.0, 10.0]
Marginal_Adhesion (float64): NaN-freq [0.0%], Samples [1.0, 1.0, 6.0, 1.0, 1.0, 5.0, 1.0, 2.0, 1.0, 8.0]
Single_Epi_Cell_Size (float64): NaN-freq [0.0%], Samples [2.0, 2.0, 10.0, 2.0, 2.0, 3.0, 2.0, 10.0, 2.0, 6.0]
Bare_Nuclei (float64): NaN-freq [0.0%], Samples [1.0, 6.0, 1.0, 1.0, 1.0, 3.0, 1.0, 10.0, 1.0, 9.0]
Bland_Chromatin (float64): NaN-freq [0.0%], Samples [2.0, 1.0, 3.0, 3.0, 3.0, 4.0, 2.0, 5.0, 2.0, 3.0]
Normal_Nucleoli (float64): NaN-freq [0.0%], Samples [2.0, 1.0, 1.0, 1.0, 1.0, 10.0, 1.0, 3.0, 1.0, 10.0]
Mitoses (float64): NaN-freq [0.0%], Samples [1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 3.0, 1.0, 10.0]
Class (category): NaN-freq [0.0%], Samples [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0]

    
This code was written by an expert datascientist working to improve predictions. It is a snippet of code that adds new columns to the dataset.
Number of samples (rows) in training dataset: 51
    
This code generates additional columns that are useful for a downstream classification algorithm (such as XGBoost) predicting "Class".
Additional columns add new semantic information, that is they use real world knowledge on the dataset. They can e.g. be feature combinations, transformations, aggregations where the new column is a function of the existing columns.
The scale of columns and offset does not matter. Make sure all used columns exist. Follow the above description of columns closely and consider the datatypes and meanings of classes.
This code also drops columns, if these may be redundant and hurt the predictive performance of the downstream classifier (Feature selection). Dropping columns may help as the chance of overfitting is lower, especially if the dataset is small.
The classifier will be trained on the dataset with the generated columns and evaluated on a holdout set. The evaluation metric is accuracy. The best performing code will be selected.
Added columns can be used in other codeblocks, dropped columns are not available anymore.

Code formatting for each added column:
```python
# (Feature name and description)
# Usefulness: (Description why this adds useful real world knowledge to classify "Class" according to dataset description and attributes.)
# Input samples: (Three samples of the columns used in the following code, e.g. 'Clump_Thickness': [5.0, 10.0, 5.0], 'Cell_Size_Uniformity': [1.0, 2.0, 6.0], ...)
(Some pandas code using Clump_Thickness', 'Cell_Size_Uniformity', ... to add a new column for each row in df)
```end

Code formatting for dropping columns:
```python
# Explanation why the column XX is dropped
df.drop(columns=['XX'], inplace=True)
```end

Each codeblock generates exactly one useful column and can drop unused columns (Feature selection).
Each codeblock ends with ```end and starts with "```python"
Codeblock:
