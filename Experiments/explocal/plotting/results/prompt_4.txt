
The dataframe `df` is loaded and in memory. Columns are also named attributes.
Description of the dataset in `df` (column dtypes might be inaccurate):
"


1. Title: Pima Indians Diabetes Database
 
    (a) Original owners: National Institute of Diabetes and Digestive and
                         Kidney Diseases
    (b) Donor of database: Vincent Sigillito (vgs@aplcen.apl.jhu.edu)
                           Research Center, RMI Group Leader
                           Applied Physics Laboratory
                           The Johns Hopkins University
                           Johns Hopkins Road
                           Laurel, MD 20707
                           (301) 953-6231
    (c) Date received: 9 May 1990
 
 3. Past Usage:
     1. Smith,~J.~W., Everhart,~J.~E., Dickson,~W.~C., Knowler,~W.~C., &
        Johannes,~R.~S. (1988). Using the ADAP learning algorithm to forecast
        the onset of diabetes mellitus.  In {it Proceedings of the Symposium
        on Computer Applications and Medical Care} (pp. 261--265).  IEEE
        Computer Society Press.
 
        The diagnostic, binary-valued variable investigated is whether the
        patient shows signs of diabetes according to World Health Organization
        criteria (i.e., if the 2 hour post-load plasma glucose was at least 
        200 mg/dl at any survey  examination or if found during routine medical
        care).   The population lives near Phoenix, Arizona, USA.
 
        Results: Their ADAP algorithm makes a real-valued prediction between
        0 and 1.  This was transformed into a binary decision using a cutoff of 
        0.448.  Using 576 training instances, the sensitivity and specificity
        of their algorithm was 76% on the remaining 192 instances.
 
 4. Relevant Information:
       Several constraints were placed on the selection of these instances from
       a larger database.  In particular, all patients here are females at
       least 21 years old of Pima Indian heritage.  ADAP is an adaptive learning
       routine that generates and executes digital analogs of perceptron-like
       devices.  It is a unique algorithm; see the paper for details.
 
 5. Number of Instances: 768
 
 6. Number of Attributes: 8 plus class 
 
 7. For Each Attribute: (all numeric-valued)
    1. Number of times pregnant
    2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test
    3. Diastolic blood pressure (mm Hg)
    4. Triceps skin fold thickness (mm)
    5. 2-Hour serum insulin (mu U/ml)
    6. Body mass index (weight in kg/(height in m)^2)
    7. Diabetes pedigree function
    8. Age (years)
    9. Class variable (0 or 1)
 
 8. Missing Attribute Values: None
 
 9. Class Distribution: (class value 1 is interpreted as "tested positive for
    diabetes")
 
    Class Value  Number of instances
    0            500
    1            268
 
 10. Brief statistical analysis:
 
     Attribute number:    Mean:   Standard Deviation:
     1.                     3.8     3.4
     2.                   120.9    32.0
     3.                    69.1    19.4
     4.                    20.5    16.0
     5.                    79.8   115.2
     6.                    32.0     7.9
     7.                     0.5     0.3
     8.                    33.2    11.8
 
 




 Relabeled values in attribute 'class'
    From: 0                       To: tested_negative     
    From: 1                       To: tested_positive"

Columns in `df` (true feature dtypes listed here, categoricals encoded as int):
preg (float64): NaN-freq [0.0%], Samples [1.0, 0.0, 1.0, 1.0, 4.0, 4.0, 6.0, 2.0, 1.0, 10.0]
plas (float64): NaN-freq [0.0%], Samples [119.0, 107.0, 115.0, 90.0, 125.0, 134.0, 87.0, 90.0, 130.0, 101.0]
pres (float64): NaN-freq [0.0%], Samples [86.0, 60.0, 70.0, 68.0, 70.0, 72.0, 80.0, 60.0, 60.0, 86.0]
skin (float64): NaN-freq [0.0%], Samples [39.0, 25.0, 30.0, 8.0, 18.0, 0.0, 0.0, 0.0, 23.0, 37.0]
insu (float64): NaN-freq [0.0%], Samples [220.0, 0.0, 96.0, 0.0, 122.0, 0.0, 0.0, 0.0, 170.0, 0.0]
mass (float64): NaN-freq [0.0%], Samples [45.6, 26.4, 34.6, 24.5, 28.9, 23.8, 23.2, 23.5, 28.6, 45.6]
pedi (float64): NaN-freq [0.0%], Samples [0.81, 0.13, 0.53, 1.14, 1.14, 0.28, 0.08, 0.19, 0.69, 1.14]
age (float64): NaN-freq [0.0%], Samples [29.0, 23.0, 32.0, 36.0, 45.0, 60.0, 32.0, 25.0, 21.0, 38.0]
class (category): NaN-freq [0.0%], Samples [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]

    
This code was written by an expert datascientist working to improve predictions. It is a snippet of code that adds new columns to the dataset.
Number of samples (rows) in training dataset: 576
    
This code generates additional columns that are useful for a downstream classification algorithm (such as XGBoost) predicting "class".
Additional columns add new semantic information, that is they use real world knowledge on the dataset. They can e.g. be feature combinations, transformations, aggregations where the new column is a function of the existing columns.
The scale of columns and offset does not matter. Make sure all used columns exist. Follow the above description of columns closely and consider the datatypes and meanings of classes.
This code also drops columns, if these may be redundant and hurt the predictive performance of the downstream classifier (Feature selection). Dropping columns may help as the chance of overfitting is lower, especially if the dataset is small.
The classifier will be trained on the dataset with the generated columns and evaluated on a holdout set. The evaluation metric is accuracy. The best performing code will be selected.
Added columns can be used in other codeblocks, dropped columns are not available anymore.

Code formatting for each added column:
```python
# (Feature name and description)
# Usefulness: (Description why this adds useful real world knowledge to classify "class" according to dataset description and attributes.)
# Input samples: (Three samples of the columns used in the following code, e.g. 'preg': [1.0, 0.0, 1.0], 'plas': [119.0, 107.0, 115.0], ...)
(Some pandas code using preg', 'plas', ... to add a new column for each row in df)
```end

Code formatting for dropping columns:
```python
# Explanation why the column XX is dropped
df.drop(columns=['XX'], inplace=True)
```end

Each codeblock generates exactly one useful column and can drop unused columns (Feature selection).
Each codeblock ends with ```end and starts with "```python"
Codeblock:
