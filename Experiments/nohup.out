python main.py --data-source-path /home/saeed/Documents/Github/CatDB/Experiments/data --data-source-name Fashion-MNIST --prompt-representation-type TEXT --prompt-example-type Random --prompt-number-example 0 --prompt-number-iteration 1 --llm-model gpt-4 --output-path /home/saeed/Documents/Github/CatDB/Experiments/catdb-results/Fashion-MNIST/gpt-4
Traceback (most recent call last):
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/main.py", line 75, in <module>
    code = generate_llm_code(model=args.llm_model, prompt=prompt_text)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/llm/GenerateLLMCode.py", line 8, in generate_llm_code
    return generate_GPT_LLM_code(model=model, prompt=prompt)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/llm/GenerateLLMCode.py", line 15, in generate_GPT_LLM_code
    completion = client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 272, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/resources/chat/completions.py", line 645, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1088, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 853, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 930, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 8192 tokens. However, you requested 106109 tokens (6109 in the messages, 100000 in the completion). Please reduce the length of the messages or completion.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
python main.py --data-source-path /home/saeed/Documents/Github/CatDB/Experiments/data --data-source-name Fashion-MNIST --prompt-representation-type TEXT --prompt-example-type Random --prompt-number-example 0 --prompt-number-iteration 1 --llm-model gpt-3.5-turbo --output-path /home/saeed/Documents/Github/CatDB/Experiments/catdb-results/Fashion-MNIST/gpt-3.5-turbo
Traceback (most recent call last):
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/main.py", line 75, in <module>
    code = generate_llm_code(model=args.llm_model, prompt=prompt_text)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/llm/GenerateLLMCode.py", line 8, in generate_llm_code
    return generate_GPT_LLM_code(model=model, prompt=prompt)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/llm/GenerateLLMCode.py", line 15, in generate_GPT_LLM_code
    completion = client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 272, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/resources/chat/completions.py", line 645, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1088, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 853, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 930, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 4097 tokens. However, your messages resulted in 6109 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
python main.py --data-source-path /home/saeed/Documents/Github/CatDB/Experiments/data --data-source-name guillermo --prompt-representation-type TEXT --prompt-example-type Random --prompt-number-example 0 --prompt-number-iteration 1 --llm-model gpt-4 --output-path /home/saeed/Documents/Github/CatDB/Experiments/catdb-results/guillermo/gpt-4
Traceback (most recent call last):
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/main.py", line 75, in <module>
    code = generate_llm_code(model=args.llm_model, prompt=prompt_text)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/llm/GenerateLLMCode.py", line 8, in generate_llm_code
    return generate_GPT_LLM_code(model=model, prompt=prompt)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/llm/GenerateLLMCode.py", line 15, in generate_GPT_LLM_code
    completion = client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 272, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/resources/chat/completions.py", line 645, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1088, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 853, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 916, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 958, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 916, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 958, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 930, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Request too large for gpt-4 in organization org-LTKwkrUwzF0sBXXfiHZ5Ydrz on tokens_usage_based per min: Limit 10000, Requested 26232. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens_usage_based', 'param': None, 'code': 'rate_limit_exceeded'}}
python main.py --data-source-path /home/saeed/Documents/Github/CatDB/Experiments/data --data-source-name guillermo --prompt-representation-type TEXT --prompt-example-type Random --prompt-number-example 0 --prompt-number-iteration 1 --llm-model gpt-3.5-turbo --output-path /home/saeed/Documents/Github/CatDB/Experiments/catdb-results/guillermo/gpt-3.5-turbo
Traceback (most recent call last):
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/main.py", line 75, in <module>
    code = generate_llm_code(model=args.llm_model, prompt=prompt_text)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/llm/GenerateLLMCode.py", line 8, in generate_llm_code
    return generate_GPT_LLM_code(model=model, prompt=prompt)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/llm/GenerateLLMCode.py", line 15, in generate_GPT_LLM_code
    completion = client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 272, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/resources/chat/completions.py", line 645, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1088, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 853, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 930, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 4097 tokens. However, your messages resulted in 38266 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
python main.py --data-source-path /home/saeed/Documents/Github/CatDB/Experiments/data --data-source-name riccardo --prompt-representation-type TEXT --prompt-example-type Random --prompt-number-example 0 --prompt-number-iteration 1 --llm-model gpt-4 --output-path /home/saeed/Documents/Github/CatDB/Experiments/catdb-results/riccardo/gpt-4
Traceback (most recent call last):
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/main.py", line 75, in <module>
    code = generate_llm_code(model=args.llm_model, prompt=prompt_text)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/llm/GenerateLLMCode.py", line 8, in generate_llm_code
    return generate_GPT_LLM_code(model=model, prompt=prompt)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/llm/GenerateLLMCode.py", line 15, in generate_GPT_LLM_code
    completion = client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 272, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/resources/chat/completions.py", line 645, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1088, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 853, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 916, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 958, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 916, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 958, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 930, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Request too large for gpt-4 in organization org-LTKwkrUwzF0sBXXfiHZ5Ydrz on tokens_usage_based per min: Limit 10000, Requested 26232. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens_usage_based', 'param': None, 'code': 'rate_limit_exceeded'}}
python main.py --data-source-path /home/saeed/Documents/Github/CatDB/Experiments/data --data-source-name riccardo --prompt-representation-type TEXT --prompt-example-type Random --prompt-number-example 0 --prompt-number-iteration 1 --llm-model gpt-3.5-turbo --output-path /home/saeed/Documents/Github/CatDB/Experiments/catdb-results/riccardo/gpt-3.5-turbo
Traceback (most recent call last):
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/main.py", line 75, in <module>
    code = generate_llm_code(model=args.llm_model, prompt=prompt_text)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/llm/GenerateLLMCode.py", line 8, in generate_llm_code
    return generate_GPT_LLM_code(model=model, prompt=prompt)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/llm/GenerateLLMCode.py", line 15, in generate_GPT_LLM_code
    completion = client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 272, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/resources/chat/completions.py", line 645, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1088, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 853, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 916, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 958, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 916, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 958, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 930, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-LTKwkrUwzF0sBXXfiHZ5Ydrz on tokens_usage_based per min: Limit 60000, Used 41092, Requested 26232. Please try again in 7.324s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens_usage_based', 'param': None, 'code': 'rate_limit_exceeded'}}
python main.py --data-source-path /home/saeed/Documents/Github/CatDB/Experiments/data --data-source-name christine --prompt-representation-type TEXT --prompt-example-type Random --prompt-number-example 0 --prompt-number-iteration 1 --llm-model gpt-4 --output-path /home/saeed/Documents/Github/CatDB/Experiments/catdb-results/christine/gpt-4
Traceback (most recent call last):
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/main.py", line 75, in <module>
    code = generate_llm_code(model=args.llm_model, prompt=prompt_text)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/llm/GenerateLLMCode.py", line 8, in generate_llm_code
    return generate_GPT_LLM_code(model=model, prompt=prompt)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/llm/GenerateLLMCode.py", line 15, in generate_GPT_LLM_code
    completion = client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 272, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/resources/chat/completions.py", line 645, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1088, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 853, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 916, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 958, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 916, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 958, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 930, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-LTKwkrUwzF0sBXXfiHZ5Ydrz on tokens_usage_based per min: Limit 10000, Used 4813, Requested 9470. Please try again in 25.698s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens_usage_based', 'param': None, 'code': 'rate_limit_exceeded'}}
python main.py --data-source-path /home/saeed/Documents/Github/CatDB/Experiments/data --data-source-name christine --prompt-representation-type TEXT --prompt-example-type Random --prompt-number-example 0 --prompt-number-iteration 1 --llm-model gpt-3.5-turbo --output-path /home/saeed/Documents/Github/CatDB/Experiments/catdb-results/christine/gpt-3.5-turbo
Traceback (most recent call last):
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/main.py", line 75, in <module>
    code = generate_llm_code(model=args.llm_model, prompt=prompt_text)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/llm/GenerateLLMCode.py", line 8, in generate_llm_code
    return generate_GPT_LLM_code(model=model, prompt=prompt)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/llm/GenerateLLMCode.py", line 15, in generate_GPT_LLM_code
    completion = client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 272, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/resources/chat/completions.py", line 645, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1088, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 853, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 930, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 4097 tokens. However, your messages resulted in 12701 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
python main.py --data-source-path /home/saeed/Documents/Github/CatDB/Experiments/data --data-source-name cnae-9 --prompt-representation-type TEXT --prompt-example-type Random --prompt-number-example 0 --prompt-number-iteration 1 --llm-model gpt-4 --output-path /home/saeed/Documents/Github/CatDB/Experiments/catdb-results/cnae-9/gpt-4
Traceback (most recent call last):
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/main.py", line 75, in <module>
    code = generate_llm_code(model=args.llm_model, prompt=prompt_text)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/llm/GenerateLLMCode.py", line 8, in generate_llm_code
    return generate_GPT_LLM_code(model=model, prompt=prompt)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/llm/GenerateLLMCode.py", line 15, in generate_GPT_LLM_code
    completion = client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 272, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/resources/chat/completions.py", line 645, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1088, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 853, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 916, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 958, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 916, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 958, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 930, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-LTKwkrUwzF0sBXXfiHZ5Ydrz on tokens_usage_based per min: Limit 10000, Used 3988, Requested 8172. Please try again in 12.96s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens_usage_based', 'param': None, 'code': 'rate_limit_exceeded'}}
python main.py --data-source-path /home/saeed/Documents/Github/CatDB/Experiments/data --data-source-name cnae-9 --prompt-representation-type TEXT --prompt-example-type Random --prompt-number-example 0 --prompt-number-iteration 1 --llm-model gpt-3.5-turbo --output-path /home/saeed/Documents/Github/CatDB/Experiments/catdb-results/cnae-9/gpt-3.5-turbo
Traceback (most recent call last):
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/main.py", line 75, in <module>
    code = generate_llm_code(model=args.llm_model, prompt=prompt_text)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/llm/GenerateLLMCode.py", line 8, in generate_llm_code
    return generate_GPT_LLM_code(model=model, prompt=prompt)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/llm/GenerateLLMCode.py", line 15, in generate_GPT_LLM_code
    completion = client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 272, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/resources/chat/completions.py", line 645, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1088, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 853, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 930, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 4097 tokens. However, your messages resulted in 6613 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
python main.py --data-source-path /home/saeed/Documents/Github/CatDB/Experiments/data --data-source-name dilbert --prompt-representation-type TEXT --prompt-example-type Random --prompt-number-example 0 --prompt-number-iteration 1 --llm-model gpt-4 --output-path /home/saeed/Documents/Github/CatDB/Experiments/catdb-results/dilbert/gpt-4
Traceback (most recent call last):
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/main.py", line 75, in <module>
    code = generate_llm_code(model=args.llm_model, prompt=prompt_text)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/llm/GenerateLLMCode.py", line 8, in generate_llm_code
    return generate_GPT_LLM_code(model=model, prompt=prompt)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/llm/GenerateLLMCode.py", line 15, in generate_GPT_LLM_code
    completion = client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 272, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/resources/chat/completions.py", line 645, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1088, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 853, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 916, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 958, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 916, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 958, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 930, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Request too large for gpt-4 in organization org-LTKwkrUwzF0sBXXfiHZ5Ydrz on tokens_usage_based per min: Limit 10000, Requested 12463. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens_usage_based', 'param': None, 'code': 'rate_limit_exceeded'}}
python main.py --data-source-path /home/saeed/Documents/Github/CatDB/Experiments/data --data-source-name dilbert --prompt-representation-type TEXT --prompt-example-type Random --prompt-number-example 0 --prompt-number-iteration 1 --llm-model gpt-3.5-turbo --output-path /home/saeed/Documents/Github/CatDB/Experiments/catdb-results/dilbert/gpt-3.5-turbo
Traceback (most recent call last):
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/main.py", line 75, in <module>
    code = generate_llm_code(model=args.llm_model, prompt=prompt_text)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/llm/GenerateLLMCode.py", line 8, in generate_llm_code
    return generate_GPT_LLM_code(model=model, prompt=prompt)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/llm/GenerateLLMCode.py", line 15, in generate_GPT_LLM_code
    completion = client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 272, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/resources/chat/completions.py", line 645, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1088, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 853, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 930, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 4097 tokens. However, your messages resulted in 17615 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
python main.py --data-source-path /home/saeed/Documents/Github/CatDB/Experiments/data --data-source-name fabert --prompt-representation-type TEXT --prompt-example-type Random --prompt-number-example 0 --prompt-number-iteration 1 --llm-model gpt-4 --output-path /home/saeed/Documents/Github/CatDB/Experiments/catdb-results/fabert/gpt-4
Traceback (most recent call last):
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/main.py", line 75, in <module>
    code = generate_llm_code(model=args.llm_model, prompt=prompt_text)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/llm/GenerateLLMCode.py", line 8, in generate_llm_code
    return generate_GPT_LLM_code(model=model, prompt=prompt)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/llm/GenerateLLMCode.py", line 15, in generate_GPT_LLM_code
    completion = client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 272, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/resources/chat/completions.py", line 645, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1088, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 853, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 916, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 958, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 916, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 958, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 930, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-LTKwkrUwzF0sBXXfiHZ5Ydrz on tokens_usage_based per min: Limit 10000, Used 2451, Requested 8172. Please try again in 3.738s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens_usage_based', 'param': None, 'code': 'rate_limit_exceeded'}}
python main.py --data-source-path /home/saeed/Documents/Github/CatDB/Experiments/data --data-source-name fabert --prompt-representation-type TEXT --prompt-example-type Random --prompt-number-example 0 --prompt-number-iteration 1 --llm-model gpt-3.5-turbo --output-path /home/saeed/Documents/Github/CatDB/Experiments/catdb-results/fabert/gpt-3.5-turbo
Traceback (most recent call last):
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/main.py", line 75, in <module>
    code = generate_llm_code(model=args.llm_model, prompt=prompt_text)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/llm/GenerateLLMCode.py", line 8, in generate_llm_code
    return generate_GPT_LLM_code(model=model, prompt=prompt)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/llm/GenerateLLMCode.py", line 15, in generate_GPT_LLM_code
    completion = client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 272, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/resources/chat/completions.py", line 645, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 1088, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 853, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/home/saeed/Documents/Github/CatDB/Experiments/setup/Baselines/CatDB/venv/lib/python3.11/site-packages/openai/_base_client.py", line 930, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 4097 tokens. However, your messages resulted in 6992 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
